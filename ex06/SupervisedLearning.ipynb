{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74f26c9",
   "metadata": {},
   "source": [
    "# Exercise 6: Supervised learning for 2D Ising model\n",
    "Before working on this notebook, generate 2D Ising data at several temperatures and save it to file. You should have written a 2D Ising simulation by now, reuse that code. Else, you may adapt one of the solutions to carry out this task.\n",
    "\n",
    "One possibility is to stringify a configuration by concatenating the rows, e.g. the configuration\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>-1</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>-1</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "could become the string \"1 1 1 -1 1 1 1 -1 1\". Then store all generated configurations in one file, and the temperatures corresponding to the different lines in a different file.\n",
    "\n",
    "Generate \"training\" data from ~20 different temperatures, ~1000 configurations per temperature, and \"verification\" data from different (and potentially more) temperatures (also roughly 1000 configurations per temperature).\n",
    "\n",
    "Think about whether it makes a difference how you represent the data (e.g. what if we concatenate the rows in a different order?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411423a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e408319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "#TODO: Add further imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15528e0b",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2c9ac700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Load your configurations for training and verification, as well as the corresponding temperatures here\n",
    "#For example, you could use np.loadtxt()\n",
    "df_train = pd.read_csv('training_set.csv')\n",
    "df_validation = pd.read_csv('validation_set.csv')\n",
    "\n",
    "training_set = df_train.to_numpy()[:,0:-1]#each row contain a stringified grid for a given temperature\n",
    "training_temp = df_train.to_numpy()[:,-1]#vector of corresponding temperatures for grid from above\n",
    "validation_set = df_validation.to_numpy()[:,0:-1]\n",
    "validation_temp = df_validation.to_numpy()[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fd566",
   "metadata": {},
   "source": [
    "# Sanity check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfabf6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_configuration(spins):\n",
    "    '''\n",
    "    This is a helper function to plot the configuration of spins given by 'spins'\n",
    "    \n",
    "    spins - an np.array of shape (N,N) with values in {-1, 1}\n",
    "    '''\n",
    "    N = np.shape(spins)[1]\n",
    "    fig, ax = plt.subplots()\n",
    "    #fig.add_axes()\n",
    "    ax = fig.axes[0]\n",
    "    for i in range(N):\n",
    "        ax.plot([i, i], [0,N-1], 'k')\n",
    "        ax.plot([0,N-1], [i,i], 'k')\n",
    "\n",
    "    colors = ['b', 'gold'] # note: blue is down, gold is up!\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            fig.gca().add_patch(plt.Circle((i,j), radius=0.35, fc=colors[int((spins[i,j]+1)/2.)]))\n",
    "   \n",
    "    ax.set_ylim(-1,N+1)\n",
    "    ax.set_xlim(-1,N+1)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('ising.png')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184583cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the above function (or one you write yourself) to check that the loaded configurations make sense\n",
    "\n",
    "test_grid = training_set[2001,:]\n",
    "test_grid = test_grid.reshape((30,30))\n",
    "print(f\"configuration for inverse temperature beta = {training_labels[2001]}\")\n",
    "plot_configuration(test_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063c5f3",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "In order for the data to be usable in training, you need to\n",
    "- Define a label coding. For an output layer with two neurons, you could code \"ordered phase\" as (1,0) and \"disordered phase\" as (0,1)\n",
    "- Generate two arrays x and y, such that x contains all configurations and y the corresponding labels, e.g. from {(1,0), (0,1)}. The result should be that the correct label of the configuration x[i] is y[i].\n",
    "- You may want to discard configurations close to the critical point (why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which removes entries from array that are in the range [0.44-ϵ, 0.44+ϵ]\n",
    "def deletion(temp,grid,epsilon):\n",
    "    indicies = np.argwhere((temp >= 0.44-epsilon) & (temp <= 0.44+epsilon))\n",
    "    new_temp = np.delete(temp,indicies,0)\n",
    "    new_grid = np.delete(grid,indicies,0)\n",
    "    return new_temp, new_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that creates labels\n",
    "#input: array of temperatures\n",
    "#output: array of same size as temp.-array with 0 if temp<0.44 and 1 if temp>0.44\n",
    "def labeler(temp_arr):\n",
    "    N = temp_arr.shape[0]\n",
    "    labels = np.zeros((N,2))\n",
    "    bigger_inidicies = np.argwhere(temp_arr > 0.44)\n",
    "    smaller_indicies = np.argwhere(temp_arr <= 0.44)\n",
    "    labels[bigger_inidicies,0] = 1\n",
    "    labels[smaller_indicies,1] = 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "42a5fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Preprocess the data\n",
    "\"\"\"so our net should only learn if the grid is ordered or disordered?\"\"\"\n",
    "\n",
    "#exclude all samples that are to close to beta = 0.44 (within epsilon range)\n",
    "training_temp, training_set = deletion(training_temp,training_set,0.02)\n",
    "#validation_temp, validation_set = deletion(validation_temp,validation_set,0.02)\n",
    "\n",
    "#create labels\n",
    "training_labels = labeler(training_temp)\n",
    "validation_labels = labeler(validation_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0047d13",
   "metadata": {},
   "source": [
    "# Construct and compile the network\n",
    "Here you construct the neural network. Follow the link given on the exercise sheet.\n",
    "\n",
    "Recommendation:\n",
    "- One hidden layer with 100 neurons and sigmoid activation, fully connected\n",
    "- Output layer with 2 neurons and softmax activation, fully connected\n",
    "\n",
    "Feel free to play around with the network setup!\n",
    "\n",
    "Finally, compile the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4aec7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 16:29:26.095802: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-12 16:29:26.096045: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#TODO: Construct the model\n",
    "num_inputs = training_set.shape[1] #number of input nodes\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=num_inputs, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad3847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eba0ee",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "829e8198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 16:29:36.101639: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-04-12 16:29:36.103874: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 16:29:36.536406: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 10s 4ms/step - loss: 0.0274 - accuracy: 0.9962\n",
      "Epoch 2/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 3/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.8618e-04 - accuracy: 1.0000\n",
      "Epoch 4/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.0005e-04 - accuracy: 1.0000\n",
      "Epoch 5/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.7169e-05 - accuracy: 1.0000\n",
      "Epoch 6/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.5455e-05 - accuracy: 1.0000\n",
      "Epoch 7/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.2949e-06 - accuracy: 1.0000\n",
      "Epoch 8/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.5527e-06 - accuracy: 1.0000\n",
      "Epoch 9/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.0300e-06 - accuracy: 1.0000\n",
      "Epoch 10/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.2569e-07 - accuracy: 1.0000\n",
      "Epoch 11/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.0012e-07 - accuracy: 1.0000\n",
      "Epoch 12/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.8741e-08 - accuracy: 1.0000\n",
      "Epoch 13/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.5732e-08 - accuracy: 1.0000\n",
      "Epoch 14/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.3967e-08 - accuracy: 1.0000\n",
      "Epoch 15/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 8.5614e-09 - accuracy: 1.0000\n",
      "Epoch 16/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.6785e-09 - accuracy: 1.0000\n",
      "Epoch 17/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.9088e-09 - accuracy: 1.0000\n",
      "Epoch 18/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.7933e-09 - accuracy: 1.0000\n",
      "Epoch 19/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.2012e-09 - accuracy: 1.0000\n",
      "Epoch 20/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.7834e-09 - accuracy: 1.0000\n",
      "Epoch 21/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.4942e-09 - accuracy: 1.0000\n",
      "Epoch 22/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.2349e-09 - accuracy: 1.0000\n",
      "Epoch 23/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.0237e-09 - accuracy: 1.0000\n",
      "Epoch 24/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 8.9515e-10 - accuracy: 1.0000\n",
      "Epoch 25/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 8.1711e-10 - accuracy: 1.0000\n",
      "Epoch 26/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 7.2760e-10 - accuracy: 1.0000\n",
      "Epoch 27/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.4497e-10 - accuracy: 1.0000\n",
      "Epoch 28/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.1284e-10 - accuracy: 1.0000\n",
      "Epoch 29/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.5316e-10 - accuracy: 1.0000\n",
      "Epoch 30/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.2332e-10 - accuracy: 1.0000\n",
      "Epoch 31/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.9348e-10 - accuracy: 1.0000\n",
      "Epoch 32/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.5676e-10 - accuracy: 1.0000\n",
      "Epoch 33/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.4758e-10 - accuracy: 1.0000\n",
      "Epoch 34/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.3381e-10 - accuracy: 1.0000\n",
      "Epoch 35/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1544e-10 - accuracy: 1.0000\n",
      "Epoch 36/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.9938e-10 - accuracy: 1.0000\n",
      "Epoch 37/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.8101e-10 - accuracy: 1.0000\n",
      "Epoch 38/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.6036e-10 - accuracy: 1.0000\n",
      "Epoch 39/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.4658e-10 - accuracy: 1.0000\n",
      "Epoch 40/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.2134e-10 - accuracy: 1.0000\n",
      "Epoch 41/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.1216e-10 - accuracy: 1.0000\n",
      "Epoch 42/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.8461e-10 - accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.6625e-10 - accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.2953e-10 - accuracy: 1.0000\n",
      "Epoch 45/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.1576e-10 - accuracy: 1.0000\n",
      "Epoch 46/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8821e-10 - accuracy: 1.0000\n",
      "Epoch 47/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8821e-10 - accuracy: 1.0000\n",
      "Epoch 48/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.7444e-10 - accuracy: 1.0000\n",
      "Epoch 49/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.6526e-10 - accuracy: 1.0000\n",
      "Epoch 50/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.6985e-10 - accuracy: 1.0000\n",
      "Epoch 51/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.5608e-10 - accuracy: 1.0000\n",
      "Epoch 52/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.4231e-10 - accuracy: 1.0000\n",
      "Epoch 53/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.3313e-10 - accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.1476e-10 - accuracy: 1.0000\n",
      "Epoch 55/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.1017e-10 - accuracy: 1.0000\n",
      "Epoch 56/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.0558e-10 - accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.0558e-10 - accuracy: 1.0000\n",
      "Epoch 58/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.0099e-10 - accuracy: 1.0000\n",
      "Epoch 59/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 9.6401e-11 - accuracy: 1.0000\n",
      "Epoch 60/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 9.1811e-11 - accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 8.7220e-11 - accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 8.7220e-11 - accuracy: 1.0000\n",
      "Epoch 63/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 8.7220e-11 - accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 8.7220e-11 - accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 8.7220e-11 - accuracy: 1.0000\n",
      "Epoch 66/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 8.7220e-11 - accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 8.2630e-11 - accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 7.3449e-11 - accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.8858e-11 - accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.8858e-11 - accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.1972e-11 - accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.1972e-11 - accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.1972e-11 - accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.1972e-11 - accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.1972e-11 - accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.1972e-11 - accuracy: 1.0000\n",
      "Epoch 77/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 8s 4ms/step - loss: 6.1972e-11 - accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.7382e-11 - accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.7382e-11 - accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.7382e-11 - accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.7382e-11 - accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.7382e-11 - accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.7382e-11 - accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.7382e-11 - accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 5.2791e-11 - accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.8201e-11 - accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.5905e-11 - accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.5905e-11 - accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.5905e-11 - accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.5905e-11 - accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.5905e-11 - accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.5905e-11 - accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.5905e-11 - accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.5905e-11 - accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.5905e-11 - accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.5905e-11 - accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.6724e-11 - accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 4.1315e-11 - accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.6724e-11 - accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.6724e-11 - accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.6724e-11 - accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.6724e-11 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.6724e-11 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.6724e-11 - accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.6724e-11 - accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.6724e-11 - accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.2134e-11 - accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 3.2134e-11 - accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.7543e-11 - accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.7543e-11 - accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.2953e-11 - accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.2953e-11 - accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.2953e-11 - accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.2953e-11 - accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.2953e-11 - accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.2953e-11 - accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 2.2953e-11 - accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 1.8362e-11 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17dafe0a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Train ('fit') the model. Experiment with number of epochs and batch sizes.\n",
    "model.fit(training_set, training_labels, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719/719 [==============================] - 2s 3ms/step - loss: 0.0087 - accuracy: 0.9984\n",
      "Accuracy: 99.84\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model on the validation set\n",
    "_, accuracy = model.evaluate(validation_set, validation_labels)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b744a990",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "Use your trained model to predict the phases of your \"validation\" configurations. Visualize the results.\n",
    "\n",
    "For example, you could predict the phase of all configurations at a certain temperature T, then count how many are classified as \"disordered\" and how many as \"ordered\". Then plot this as a function of T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f259cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Perform predictions, visualize the results and interpret.\n",
    "predictions = (model.predict(validation_set) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_temp = int(validation_temp.shape[0]/1000) #number of different temperatures in the validation_temp\n",
    "counter = []\n",
    "temperatures = []\n",
    "\n",
    "#for each temp in validation_temp count how many configurations are in the ordered state and store in counter\n",
    "for i in range(num_temp):\n",
    "    temp = validation_temp[i*1000]\n",
    "    indicies = np.argwhere(validation_temp == temp)\n",
    "    predic = predictions[indicies,:]\n",
    "    #number of ordered states:\n",
    "    ordered_states = np.argwhere(predic == [[1,0]]).shape[0]\n",
    "    counter = np.append(counter,ordered_states)\n",
    "    temperatures = np.append(temperatures, temp)\n",
    "\n",
    "counter /= 2000 #normalize the counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#plot the number of ordered states over the temperature\n",
    "plt.scatter(temperatures, counter, color='blue', linewidth=1, label='predictions')\n",
    "plt.plot(temperatures, counter, color='darkorange', linewidth=2.5)\n",
    "plt.xlabel(r\"inverse temperature $\\beta$\")\n",
    "plt.ylabel(\"percentage of ordered states in prediction\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('Ordered_states.svg', format = 'svg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
